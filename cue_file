#!/usr/bin/env python3
# encoding: utf-8

# cue_file
# 2024-03-23 Moonbase59
#
# Originally based on an idea and some code by John Warburton (@Warblefly):
#   https://github.com/Warblefly/TrackBoundaries

# TODO: Allow reading tags and possibly return a partial result if these are incomplete.
# Maybe use ffmpeg for this:
#   ffmpeg -i file.ext -f ffmetadata - 2>/dev/null | grep -v '^;'
# and split key=value pairs.
# For partial result JSON, the Liquidsoap protocol must also be modified.

# TODO: Allow writing tags. Difficult, because ffmpeg messes up FLAC tags at least.
# (Concatenates multiple values into one tag, separated by ';'.)
# Could use Mutagen, but need to differentiate lots of file types in this case,
# plus have Mutagen installed in AzuraCast Docker.

import subprocess
import argparse
import json
import re

# Default presets
FFMPEG = "ffmpeg"  # location of the ffmpeg binary
TARGET_LUFS = -18.0  # Reference Loudness Target
# -96 dB/LU is "digital silence" for 16-bit audio.
# A "noise floor" of -60 dB/LU (42 dB/LU below -18 target) is a good value to use.
SILENCE = -42.0  # LU below average for cue-in/cue-out trigger ("silence")
OVERLAY_LU = -8.0  # LU below average for overlay trigger (start next song)
# more than LONGTAIL_SECONDS below OVERLAY_LU are considered a "long tail"
LONGTAIL_SECONDS = 15.0
LONGTAIL_EXTRA_LU = -15.0  # reduce 15 dB extra on long tail songs to find overlap point


def analyse(
        filename,
        target=TARGET_LUFS,
        overlay=OVERLAY_LU,
        silence=SILENCE,
        longtail_seconds=LONGTAIL_SECONDS,
        extra=LONGTAIL_EXTRA_LU,
        blankskip=False):
    # ffmpeg -v quiet -i audiofile.ext -vn -af ebur128=target=-18:metadata=1,ametadata=mode=print:file=- -f null null
    # Output:
    # frame:448  pts:2150400 pts_time:44.8
    # lavfi.r128.M=-78.490
    # lavfi.r128.S=-78.566
    # lavfi.r128.I=-18.545
    # lavfi.r128.LRA=5.230
    # lavfi.r128.LRA.low=-23.470
    # lavfi.r128.LRA.high=-18.240

    result = subprocess.run(
        [
            FFMPEG,
            "-v",
            "quiet",
            "-y",
            "-i",
            filename,
            "-vn",
            "-af",
            "ebur128=target=" +
            str(target) +
            ":metadata=1,ametadata=mode=print:file=-",
            "-f",
            "null",
            "null"],
        stdout=subprocess.PIPE,
        # stderr=subprocess.STDOUT,
        check=True,
        text=True).stdout

    measure = []
    # Extract time "t", momentary (last 400ms) loudness "M" and "I" integrated loudness
    # from ebur128 filter. Measured every 100ms.
    # With some file types, like MP3, M can become "nan" (not-a-number),
    # which is a valid float in Python. Usually happens on very silent parts.
    # FIXME: This relies on "I" coming two lines after "M"
    pattern = re.compile(
        r"frame:.*pts_time:\s*(?P<t>\d+\.?\d*)\s*lavfi\.r128\.M=(?P<M>nan|[+-]?\d+\.?\d*)\s*.*\s*lavfi\.r128\.I=(?P<I>nan|[+-]?\d+\.?\d*)",
        flags=re.M)

    for match in re.finditer(pattern, result):
        m = match.groupdict()
        measure.append([float(m["t"]), float(m["M"]), float(m["I"])])

    # range to watch (for later blank skip)
    start = 0
    end = len(measure)

    # get actual duration from last PTS (Presentation Time Stamp)
    # This is the last frame, so the total duration is its PTS + frame length
    # (100ms)
    duration = measure[end-1][0] + 0.1

    # get integrated song loudness from last frame, so we can calculate liq_amplify
    # (the "ReplayGain") from it (difference to desired loudness target)
    loudness = measure[end-1][2]

    # Find cue-in point (loudness above "silence")
    silence_level = loudness + silence
    cue_in_time = 0.0
    for i in range(start, end):
        if measure[i][1] > silence_level:
            cue_in_time = measure[i][0]
            start = i
            break
    # EBU R.128 measures loudness over the last 400ms,
    # adjust to zero if we land before 400ms for cue-in
    cue_in_time = 0.0 if cue_in_time < 0.4 else cue_in_time

    # Instead of simply reversing the list (measure.reverse()), we henceforth
    # use "start" and "end" pointers into the measure list, so we can easily
    # check forwards and backwards, and handle partial ranges better.
    # This is mainly for early cue-outs due to blanks in file ("hidden tracks"),
    # as we need to handle overlaying and long tails correctly in this case.

    cue_out_time = 0.0
    cue_out_time_blank = 0.0

    # Cue-out when silence starts within a song, like "hidden tracks".
    # Check forward in this case, and trust EBU R128’s 400ms to be long enough,
    # checking for loudness going below the defined silence level.
    # NOTE: This shouldn’t be used with TTS-generated jingles and spoken text,
    # because the pauses in speech will trigger the detection and cut off the text!
    if blankskip:
        # print("Checking for blank")
        end_blank = end
        for i in range(start, end):
            if measure[i][1] <= silence_level:
                cue_out_time_blank = measure[i][0]
                end_blank = i+1
                # print(f"Found cue-out blank: {end_blank}, {cue_out_time_blank}")
                break

    # Normal cue-out: check backwards, from the end, for loudness above "silence"
    for i in reversed(range(start, end)):
        if measure[i][1] > silence_level:
            cue_out_time = measure[i][0]
            end = i+1
            # print(f"Found cue-out: {end}, {cue_out_time}")
            break
    # cue out PAST the current frame (100ms) -- no, reverse that
    cue_out_time = max(cue_out_time, duration - cue_out_time)

    # Adjust cue-out and "end" point if we're working with blank detection.
    # Also set a flag (`liq_blank_skipped`) so we can later see if cue-out is early.
    blank_skipped = False
    if blankskip:
        # cue out PAST the current frame (100ms) -- no, reverse that
        #cue_out_time_blank = cue_out_time_blank + 0.1
        # print(f"cue-out blank: {cue_out_time_blank}, cue-out: {cue_out_time}")
        if 0.0 < cue_out_time_blank < cue_out_time:
            cue_out_time = cue_out_time_blank
            blank_skipped = True
        end = end_blank

    # Find overlap point (where to start next song), backwards from end,
    # by checking if song loudness goes below overlay start volume
    cue_duration = cue_out_time - cue_in_time
    start_next_level = loudness + overlay
    start_next_time = 0.0
    for i in reversed(range(start, end)):
        if measure[i][1] > start_next_level:
            start_next_time = measure[i][0]
            break
    start_next_time = max(start_next_time, cue_out_time - start_next_time)

    # We want to keep songs with a long fade-out intact, so if the calculated
    # overlap is longer than the "longtail_seconds" time, we check again, by reducing
    # the loudness to look for by an additional "extra" amount of LU
    longtail = False

    if (cue_out_time - start_next_time) > longtail_seconds:
        longtail = True
        start_next_level = loudness + overlay + extra
        start_next_time = 0.0
        for i in reversed(range(start, end)):
            if measure[i][1] > start_next_level:
                start_next_time = measure[i][0]
                break
        start_next_time = max(start_next_time, cue_out_time - start_next_time)

    # return a dict
    return ({
        "duration": duration,
        "liq_cue_in": cue_in_time,
        "liq_cue_out": cue_out_time,
        "liq_longtail": longtail,
        "liq_cross_start_next": start_next_time,
        "liq_loudness": loudness,
        "liq_amplify": (target - loudness),
        "liq_blank_skipped": blank_skipped
    })


# CLI command parser and help text
parser = argparse.ArgumentParser(
    description="Return cue-in, cue-out, overlay and replaygain data for an audio " \
    "file as JSON. To be used with my Liquidsoap \"autocue:\" protocol.",
    epilog="Please report any issues to https://github.com/Moonbase59/autocue/issues",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)

parser.add_argument("file", help="File to be processed")
parser.add_argument("-t", "--target", help="LUFS reference target",
                    default=TARGET_LUFS, type=int, choices=range(-23, 1))
parser.add_argument(
    "-s",
    "--silence",
    help="LU/dB below integrated track loudness for cue-in & cue-out points " \
    "(silence removal at beginning & end of a track)",
    default=SILENCE,
    type=int)
parser.add_argument(
    "-o",
    "--overlay",
    help="LU/dB below integrated track loudness to trigger next track",
    default=OVERLAY_LU,
    type=int)
parser.add_argument(
    "-l",
    "--longtail",
    help="More than so many seconds of calculated overlay duration are considered " \
    "a long tail, and will force a recalculation using --extra, thus keeping long " \
    "song endings intact",
    default=LONGTAIL_SECONDS,
    type=float)
parser.add_argument(
    "-x",
    "--extra",
    help="Extra LU/dB below overlay loudness to trigger next track for songs " \
    "with long tail",
    default=LONGTAIL_EXTRA_LU,
    type=int)
parser.add_argument(
    "-b",
    "--blankskip",
    help="Skip blank (silence) within song (get rid of \"hidden tracks\"). " \
    "Sets the cue-out point to where the silence begins. Don't use this with " \
    "spoken or TTS-generated text, as it will often cut the message short.",
    default=False,
    action='store_true')
parser.add_argument(
    "-w",
    "--write",
    help="Write Liquidsoap tags to file",
    default=False,
    action='store_true')
parser.add_argument(
    "-f",
    "--force",
    help="Force re-calculation, even if tags exist",
    default=False,
    action='store_true')

args = parser.parse_args()

result = analyse(
    filename=args.file,
    target=args.target,
    overlay=args.overlay,
    silence=args.silence,
    longtail_seconds=args.longtail,
    extra=args.extra,
    blankskip=args.blankskip
)

# prepare string-based JSON result
# we use "dB" instead of "LU" units, because LS & others don’t understand "LU"
liq_result = {
    "duration": result['duration'],
    "liq_cue_in": result['liq_cue_in'],
    "liq_cue_out": result['liq_cue_out'],
    "liq_longtail": result["liq_longtail"],
    "liq_cross_start_next": result['liq_cross_start_next'],
    "liq_loudness": f"{result['liq_loudness']:.2f} dB",
    "liq_amplify": f"{result['liq_amplify']:.2f} dB",
    "liq_blank_skipped": result["liq_blank_skipped"],
}

# output compact (one line) JSON, for use in Liquidsoap "autocue:" protocol
json_output = json.dumps(liq_result)
print(json_output)
